{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anghel Fabian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "#python packages\n",
    "\n",
    "#Bert Model\n",
    "from transformers import pipeline, AutoTokenizer, BertForMaskedLM\n",
    "unmasker = pipeline('fill-mask', model='dumitrescustefan/bert-base-romanian-cased-v1', top_k=10)\n",
    "\n",
    "#tokenizer\n",
    "import spacy\n",
    "NLP = spacy.load(\"ro_core_news_lg\")\n",
    "\n",
    "#text classification\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('ro', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.ro.300.bin')\n",
    "\n",
    "#zipf_score\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "#cosine similarity\n",
    "import scipy.spatial, scipy.special\n",
    "\n",
    "#min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "synonyms_dict = {}\n",
    "antonyms_dict = {}\n",
    "\n",
    "#load synonyms and antonyms\n",
    "#with open(join(dirname(__file__), 'synonyms.json'), 'r') as f:\n",
    "with open('scraping/synonyms.json', 'r') as f:\n",
    "    synonyms_dict = json.load(f)\n",
    "\n",
    "#with open(join(dirname(__file__), 'antonyms.json'), 'r') as f:\n",
    "with open('scraping/antonyms.json', 'r') as f:\n",
    "    antonyms_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dex online data\n",
    "with open('dex-online-database/word_inflections.json', 'r') as f:\n",
    "    word_inflections = json.load(f)\n",
    "\n",
    "with open('dex-online-database/spaCy_tags.json', 'r') as f:\n",
    "    spacy_tags = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    #get right versions of ș/ț\n",
    "    sentence = sentence.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\")\n",
    "\n",
    "    #transform a/i + unicode 770/u+0302 in â/î\n",
    "    sentence = sentence.replace(\"a\\u0302\", \"â\").replace(\"i\\u0302\", \"î\").replace(\"A\\u0302\", \"Â\").replace(\"I\\u0302\", \"Î\")\n",
    "\n",
    "    #transform s/t + unicode 807/u+0327 or 806/u+0326 in ș/ț\n",
    "    sentence = sentence.replace(\"s\\u0327\", \"ș\").replace(\"t\\u0327\", \"ț\").replace(\"S\\u0327\", \"Ș\").replace(\"T\\u0327\", \"Ț\")\n",
    "    sentence = sentence.replace(\"s\\u0326\", \"ș\").replace(\"t\\u0326\", \"ț\").replace(\"S\\u0326\", \"Ș\").replace(\"T\\u0326\", \"Ț\")\n",
    "\n",
    "    #transform a + unicode 774/u+0306 in ă\n",
    "    sentence = sentence.replace(\"a\\u0306\", \"ă\").replace(\"A\\u0307\", \"Ă\")\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_token(token, zipf_score):\n",
    "    #if token is punctuation\n",
    "    if token.is_punct:\n",
    "        return False\n",
    "    \n",
    "    #if token is number\n",
    "    if token.is_digit or token.like_num:\n",
    "        return False\n",
    "    \n",
    "    #if token is whitespace\n",
    "    if token.is_space:\n",
    "        return False\n",
    "    \n",
    "    #if token is currency:\n",
    "    if token.is_currency:\n",
    "        return False\n",
    "    \n",
    "    #if token is url or email\n",
    "    if token.like_url or token.like_email:\n",
    "        return False\n",
    "    \n",
    "    #should be replaced only tokens that are nouns, adjectives, adverbs or verbs (proper nouns are also excluded here)\n",
    "    if token.pos_ not in [\"NOUN\", \"ADJ\", \"ADV\", \"VERB\", \"AUX\"]:\n",
    "        return False\n",
    "    \n",
    "    #if token is proper noun\n",
    "    if token.pos_ == \"NOUN\" and token.tag_[1] == 'p':\n",
    "        return False\n",
    "    \n",
    "    #too high zipf score\n",
    "    if zipf_score > 4.25:\n",
    "    #if zipf_score > 4.5:\n",
    "    #if zipf_score > 4:\n",
    "        return False\n",
    "    \n",
    "    #to do: check ENTs\n",
    "    #some can be general words (like \"îmbrăcăminte\", \"litoral\", \"alcool\")\n",
    "    #DATETIME should still be replaced\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_candidates(sentence, new_tokens):\n",
    "    tokens = NLP(sentence)\n",
    "    replacable_tokens = []\n",
    "\n",
    "    #sort tokens by Zipf score\n",
    "    for token in tokens:\n",
    "        #token_score = zipf_frequency(token.text, 'ro')\n",
    "        token_score = zipf_frequency(token.lemma_, \"ro\")\n",
    "        if valid_token(token, token_score) and token.text not in new_tokens:\n",
    "            #context = extract_context(tokens, token.i)\n",
    "            replacable_tokens.append((token, token_score))\n",
    "\n",
    "    replacable_tokens.sort(key=lambda x: x[1])\n",
    "\n",
    "    return replacable_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitution_generation(sentence, token):\n",
    "    # generate a masked sentence in this form: [CLS] original sentence [SEP] masked sentence [SEP]\n",
    "    masked_sentence = sentence[:token.idx] + \"[MASK]\" + sentence[token.idx + len(token.text):]\n",
    "    model_sentence = \"[CLS] \" + sentence + \" [SEP] \" + masked_sentence + \" [SEP]\"\n",
    "    result = unmasker(model_sentence)\n",
    "    suggestions = []\n",
    "\n",
    "    for x in result:\n",
    "        #check if suggestion is same part of speech \n",
    "        replaced_sentence = sentence[:token.idx] + x[\"token_str\"] + sentence[token.idx + len(token.text):]\n",
    "        replaced_tokens = NLP(replaced_sentence)\n",
    "        suggestion_token = replaced_tokens[token.i]\n",
    "\n",
    "        if token.pos != suggestion_token.pos and not (token.pos_ == \"AUX\" and suggestion_token.pos_ == \"VERB\") and not (token.pos_ == \"VERB\" and suggestion_token.pos_ == \"AUX\"):\n",
    "            continue\n",
    "\n",
    "        suggestions.append(x)\n",
    "\n",
    "    return suggestions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_score(suggestion, token):\n",
    "    sugg_token = NLP(suggestion)[0]\n",
    "    sugg_lemma = sugg_token.lemma_\n",
    "    token_lemma = token.lemma_\n",
    "\n",
    "    #positive score if synonyms\n",
    "    if (token_lemma in synonyms_dict and sugg_lemma in synonyms_dict[token_lemma]) or (sugg_lemma in synonyms_dict and token_lemma in synonyms_dict[sugg_lemma]):\n",
    "        return 1\n",
    "\n",
    "    #negative score if antonyms\n",
    "    if (token_lemma in antonyms_dict and sugg_lemma in antonyms_dict[token_lemma]) or (sugg_lemma in antonyms_dict and token_lemma in antonyms_dict[sugg_lemma]):\n",
    "        return 0\n",
    "    if \"ne\" + token_lemma == sugg_lemma or \"ne\" + sugg_lemma == token_lemma:\n",
    "        return 0\n",
    "    if \"in\" + token_lemma == sugg_lemma or \"in\" + sugg_lemma == token_lemma:\n",
    "        return 0\n",
    "    \n",
    "    #negative score if same word to encourage different suggestions\n",
    "    if token_lemma == sugg_lemma:\n",
    "        return 0.25\n",
    "    \n",
    "    #netural score if not related\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitution_ranking(suggetions, token):\n",
    "    substitutions = []\n",
    "\n",
    "    bert_scores = []\n",
    "    zipf_scores = []\n",
    "    fasttext_scores = []\n",
    "    synonym_scores = []\n",
    "\n",
    "    # calculate a score for each suggestion based on BERT, Zipf and FastText\n",
    "    for x in suggetions:\n",
    "        bert_scores.append([x[\"score\"]])\n",
    "\n",
    "        zipf_score = zipf_frequency(x[\"token_str\"], 'ro')\n",
    "        zipf_scores.append([zipf_score])\n",
    "\n",
    "        fasttext_score = 1 - scipy.spatial.distance.cosine(ft[token.text], ft[x[\"token_str\"]])\n",
    "        fasttext_scores.append([fasttext_score])\n",
    "\n",
    "        syn_score = synonym_score(x[\"token_str\"], token)\n",
    "        synonym_scores.append([syn_score])\n",
    "    \n",
    "    if len(bert_scores) > 0:\n",
    "        bert_scaler = MinMaxScaler()\n",
    "        zipf_scaler = MinMaxScaler()\n",
    "        fasttext_scaler = MinMaxScaler()\n",
    "        synonym_scaler = MinMaxScaler()\n",
    "\n",
    "        bert_scores_norm = bert_scaler.fit_transform(bert_scores)\n",
    "        zipf_scores_norm = zipf_scaler.fit_transform(zipf_scores)\n",
    "        fasttext_scores_norm = fasttext_scaler.fit_transform(fasttext_scores)\n",
    "        synonym_scores_norm = synonym_scaler.fit_transform(synonym_scores)\n",
    "\n",
    "        for i in range(len(suggetions)):\n",
    "            final_score = (bert_scores_norm[i] + zipf_scores_norm[i] + fasttext_scores_norm[i] + synonym_scores[i]) / 4\n",
    "            substitutions.append((suggetions[i][\"token_str\"], final_score))\n",
    "\n",
    "        # sort suggestions by score and replace the word with the best suggestion if it has a higher Zipf score\n",
    "        substitutions.sort(reverse=True, key=lambda x: x[1])\n",
    "        #print(substitutions)\n",
    "\n",
    "    return substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_form(token, suggestion):\n",
    "    #search if original word has inflections available for its' pos tag\n",
    "    has_inflections = False\n",
    "    inflection_ids = []\n",
    "\n",
    "    if token.lemma_ in word_inflections:\n",
    "        if token.tag_ in spacy_tags:\n",
    "            has_inflections = True\n",
    "            inflection_ids = spacy_tags[token.tag_][\"dex_online_id\"]\n",
    "\n",
    "    #replace the top replacement with the right form if found\n",
    "    suggestion_lemma = NLP(suggestion)[0].lemma_\n",
    "\n",
    "    if has_inflections and suggestion_lemma in word_inflections:\n",
    "        for id in inflection_ids:\n",
    "            str_id = str(id)\n",
    "            if str_id in word_inflections[suggestion_lemma][\"inflections\"]:\n",
    "                if token.is_title:\n",
    "                    return word_inflections[suggestion_lemma][\"inflections\"][str_id].title()\n",
    "                return word_inflections[suggestion_lemma][\"inflections\"][str_id]\n",
    "\n",
    "    #if not replacement is found return original word\n",
    "    if token.is_title:\n",
    "        return suggestion.title()\n",
    "    return suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_simplification(sentence):\n",
    "    token_replaced = True\n",
    "    new_tokens = set([])\n",
    "\n",
    "    #preprocessing\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    #sent_tokens = NLP(sentence)\n",
    "    #for token in sent_tokens:\n",
    "        #if token.ent_iob != 2:\n",
    "            #print(token.text, token.ent_type_)\n",
    "        #print(token.text, token.pos_, token.tag_)\n",
    "\n",
    "    while token_replaced:\n",
    "        token_replaced = False\n",
    "\n",
    "        replacable_tokens = word_candidates(sentence, new_tokens)\n",
    "        \n",
    "        for token in replacable_tokens:\n",
    "            if token_replaced:\n",
    "                break\n",
    "\n",
    "            token_text = token[0].text\n",
    "\n",
    "            suggestions = substitution_generation(sentence, token[0])\n",
    "            suggestions = substitution_ranking(suggestions, token[0])\n",
    "\n",
    "            if len(suggestions) == 0:\n",
    "                continue\n",
    "            \n",
    "            top_replacement = suggestions[0][0]\n",
    "\n",
    "            if zipf_frequency(top_replacement, 'ro') >= zipf_frequency(token_text, 'ro'):\n",
    "                top_replacement = align_word_form(token[0], top_replacement)\n",
    "                sentence = sentence[:token[0].idx] + top_replacement + sentence[token[0].idx + len(token_text):]\n",
    "                new_tokens.add(top_replacement)\n",
    "                token_replaced = True\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_simplification(text):\n",
    "    doc = NLP(text)\n",
    "    for sentence in doc.sents:\n",
    "        new_sentence = sentence_simplification(sentence.text)\n",
    "        text = text.replace(sentence.text, new_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marea era spectaculoasă și se întindea până la orizont.\n",
      "Ei avuseră prea mult alcool în acea seară.\n",
      "Mihai a mers la mall ca să își cumpere haină.\n",
      "Eu și gașca mea am mers în vacanță la mare.\n",
      "Am mâncat prea multă ciocolată și am făcut diabet.\n",
      "La facultate am învățat despre structuri și structuri de date.\n",
      "Articolul publicat de mine a fost citit de toată lumea.\n"
     ]
    }
   ],
   "source": [
    "#normal sentences\n",
    "sentence_list = [\"Priveliștea era spectaculoasă și se întindea până la orizont.\", \n",
    "                 \"Ei consumaseră prea mult alcool în acea seară.\", \n",
    "                 \"Mihai a mers la mall ca să își achiziționeze îmbrăcăminte.\",\n",
    "                 \"Eu și gașca mea am mers în vacanță la litoral.\",\n",
    "                 \"Am mâncat prea multă ciocolată și am făcut hiperglicemie.\", \n",
    "                 \"La facultate am studiat despre algoritmi și structuri de date.\",\n",
    "                 \"Articolul publicat de mine a fost citit de toată lumea.\",]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    print(sentence_simplification(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creșterea constantă în fața responsabilităților academice poate duce la o creștere a depresiei și la pierderea performanțelor intelectuale pe termen lung.\n",
      "Implementarea unor strategii didactice inovatoare poate facilita utilizarea cunoștințelor teoretice într-un mod eficient și sustenabil.\n",
      "Pierderea echilibrului ecologic cauzată de defriș ilegale are consecințe grave asupra biodiversității și mediului global.\n",
      "Promovarea informațiilor eronate prin canale digitale poate conduce la manipulare masivă și la pierderea încrederii publice în instituțiile fundamentale.\n",
      "Calitatea proceselor administrative influențează negativ eficiența deciziilor guvernamentale.\n",
      "Presiunea ideologică din societate poate compromite coeziunea comunitară și agrava conflictele existente.\n"
     ]
    }
   ],
   "source": [
    "#complex sentences\n",
    "sentence_list = [\"Procrastinarea constantă în fața responsabilităților academice poate duce la o exacerbare a anxietății și la compromiterea performanțelor intelectuale pe termen lung.\",\n",
    "                 \"Implementarea unor strategii pedagogice inovatoare poate facilita internalizarea cunoștințelor teoretice într-un mod eficient și sustenabil.\",\n",
    "                 \"Perturbarea echilibrului ecologic cauzată de defrișările necontrolate generează consecințe ireversibile asupra biodiversității și climatului global.\",\n",
    "                 \"Diseminarea informațiilor eronate prin canale digitale poate conduce la dezinformare masivă și la erodarea încrederii publice în instituțiile fundamentale.\",\n",
    "                 \"Complexitatea proceselor administrative influențează negativ eficiența deciziilor guvernamentale.\",\n",
    "                 \"Polarizarea ideologică din societate poate compromite coeziunea comunitară și exacerba conflictele latente.\"]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    print(sentence_simplification(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au trecut peste 147 de ani de la înființarea Universității din București, iar eu am studiat acolo trei ani.\n",
      "Găsiți mai multe informații pe https://spacy.io/api/token.\n"
     ]
    }
   ],
   "source": [
    "#special sentences\n",
    "sentence_list = [\"Au trecut peste 147 de ani de la înființarea Universității din București, iar eu am studiat acolo trei ani.\", \n",
    "                 \"Găsiți mai multe informații pe https://spacy.io/api/token.\",]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    print(sentence_simplification(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "În contextul actual al dezvoltării societății contemporane, se observă o multitudine de probleme emergente care influențează semnificativ calitatea vieții cotidiene. Integrarea factorilor relevanți contribuie la creșterea calității soluțiilor propuse pentru ameliorarea acestor deficiențe. De exemplu, în domeniul educației, implementarea unor strategii didactice elaborat și utilizarea unor metode avansate au devenit priorități esențiale. Totuși, o parte considerabilă a populației rămâne pierdută din cauza lipsei disponibilității resurselor educaționale. Pe de altă parte, în sectorul economic, dinamica pieței și globalizarea contribuie la apariția unor provocări importante pentru întreprinderile mici și mijlocii. În acest sens, este esențial să fie identificat modalități eficiente pentru creșterea competitivității acestor organizații. Un alt aspect important este cel legat de sănătate. Progresu tehnologic a permis crearea unor dispozitive medicale noi, însă accesul la acestea este, din păcate, limitat pentru o parte semnificativă a populației.\n"
     ]
    }
   ],
   "source": [
    "text = \"În contextul actual al dezvoltării societății contemporane, se observă o multitudine de problematici emergente care influențează semnificativ calitatea vieții cotidiene. Diversitatea factorilor determinanți contribuie la creșterea complexității soluțiilor propuse pentru ameliorarea acestor deficiențe. De exemplu, în domeniul educației, implementarea unor strategii pedagogice elaborate și utilizarea unor metodologii avansate au devenit priorități esențiale. Totuși, o parte considerabilă a populației rămâne marginalizată din cauza lipsei accesibilității resurselor educaționale. Pe de altă parte, în sectorul economic, dinamica pieței și globalizarea contribuie la apariția unor provocări substanțiale pentru întreprinderile mici și mijlocii. În acest sens, este imperativ să fie identificate modalități eficiente pentru stimularea sustenabilității acestor organizații. Un alt aspect relevant este cel legat de sănătate. Progresul tehnologic a facilitat crearea unor dispozitive medicale inovative, însă accesul la acestea este, din păcate, limitat pentru o parte semnificativă a populației.\"\n",
    "\n",
    "print(text_simplification(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
