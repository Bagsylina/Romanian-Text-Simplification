{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anghel Fabian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#python packages\n",
    "\n",
    "#Bert Model\n",
    "from transformers import pipeline, AutoTokenizer, BertForMaskedLM\n",
    "unmasker = pipeline('fill-mask', model='dumitrescustefan/bert-base-romanian-cased-v1', top_k=10)\n",
    "\n",
    "#tokenizer\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "NLP = spacy.load(\"ro_core_news_lg\")\n",
    "\n",
    "#text classification\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('ro', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.ro.300.bin')\n",
    "\n",
    "#zipf_score\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "#cosine similarity\n",
    "import scipy.spatial, scipy.special\n",
    "\n",
    "#min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "synonyms_dict = {}\n",
    "antonyms_dict = {}\n",
    "\n",
    "#load synonyms and antonyms\n",
    "#with open(join(dirname(__file__), 'synonyms.json'), 'r') as f:\n",
    "with open('scraping/synonyms.json', 'r') as f:\n",
    "    synonyms_dict = json.load(f)\n",
    "\n",
    "#with open(join(dirname(__file__), 'antonyms.json'), 'r') as f:\n",
    "with open('scraping/antonyms.json', 'r') as f:\n",
    "    antonyms_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dex online data\n",
    "with open('dex-online-database/word_inflections.json', 'r') as f:\n",
    "    word_inflections = json.load(f)\n",
    "\n",
    "with open('dex-online-database/spaCy_tags.json', 'r') as f:\n",
    "    spacy_tags = json.load(f)\n",
    "\n",
    "with open('dex-online-database/pron_and_num_pairs.json', 'r', encoding='utf-8') as f:\n",
    "    pron_pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    #get right versions of ș/ț\n",
    "    sentence = sentence.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\")\n",
    "\n",
    "    #transform a/i + unicode 770/u+0302 in â/î\n",
    "    sentence = sentence.replace(\"a\\u0302\", \"â\").replace(\"i\\u0302\", \"î\").replace(\"A\\u0302\", \"Â\").replace(\"I\\u0302\", \"Î\")\n",
    "\n",
    "    #transform s/t + unicode 807/u+0327 or 806/u+0326 in ș/ț\n",
    "    sentence = sentence.replace(\"s\\u0327\", \"ș\").replace(\"t\\u0327\", \"ț\").replace(\"S\\u0327\", \"Ș\").replace(\"T\\u0327\", \"Ț\")\n",
    "    sentence = sentence.replace(\"s\\u0326\", \"ș\").replace(\"t\\u0326\", \"ț\").replace(\"S\\u0326\", \"Ș\").replace(\"T\\u0326\", \"Ț\")\n",
    "\n",
    "    #transform a + unicode 774/u+0306 in ă\n",
    "    sentence = sentence.replace(\"a\\u0306\", \"ă\").replace(\"A\\u0307\", \"Ă\")\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_token(token, zipf_score):\n",
    "    #if token is punctuation\n",
    "    if token.is_punct:\n",
    "        return False\n",
    "    \n",
    "    #if token is number\n",
    "    if token.is_digit or token.like_num:\n",
    "        return False\n",
    "    \n",
    "    #if token is whitespace\n",
    "    if token.is_space:\n",
    "        return False\n",
    "    \n",
    "    #if token is currency:\n",
    "    if token.is_currency:\n",
    "        return False\n",
    "    \n",
    "    #if token is url or email\n",
    "    if token.like_url or token.like_email:\n",
    "        return False\n",
    "    \n",
    "    #should be replaced only tokens that are nouns, adjectives, adverbs or verbs (proper nouns are also excluded here)\n",
    "    if token.pos_ not in [\"NOUN\", \"ADJ\", \"ADV\", \"VERB\", \"AUX\"]:\n",
    "        return False\n",
    "    \n",
    "    #if token is proper noun\n",
    "    if token.pos_ == \"NOUN\" and token.tag_[1] == 'p':\n",
    "        return False\n",
    "    \n",
    "    #exclude capitalised words if not at the start of a sentence (it's a name)\n",
    "    if not token.is_sent_start and token.text[0].isupper():\n",
    "        return False\n",
    "\n",
    "    #exclude cardinal directions\n",
    "    cardinal_directions = [\"nord\", \"est\", \"sud\", \"vest\", \"nord-est\", \"nord-vest\", \"sud-est\", \"sud-vest\", \"nordic\", \"estic\", \"sudic\", \"vestic\", \"nord-estic\", \"nord-vestic\", \"sud-estic\", \"sud-vestic\"]\n",
    "    if token.lemma_ in cardinal_directions:\n",
    "        return False\n",
    "\n",
    "    #exclude stop words (should not be replaced as they are frequently used words)\n",
    "    if token.is_stop:\n",
    "        return False\n",
    "    \n",
    "    #too high zipf score\n",
    "    #if zipf_score > 4.25:\n",
    "    if zipf_score > 4.5:\n",
    "    #if zipf_score > 4:\n",
    "        return False\n",
    "    \n",
    "    #to do: check ENTs\n",
    "    #some can be general words (like \"îmbrăcăminte\", \"litoral\", \"alcool\")\n",
    "    #DATETIME should still be replaced\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_candidates(sentence, new_tokens):\n",
    "    tokens = NLP(sentence)\n",
    "    replacable_tokens = []\n",
    "\n",
    "    #sort tokens by Zipf score\n",
    "    for token in tokens:\n",
    "        #token_score = zipf_frequency(token.text, 'ro')\n",
    "        token_score = zipf_frequency(token.lemma_, \"ro\")\n",
    "        if valid_token(token, token_score) and token.text not in new_tokens:\n",
    "            #context = extract_context(tokens, token.i)\n",
    "            replacable_tokens.append((token, token_score))\n",
    "\n",
    "    replacable_tokens.sort(key=lambda x: x[1])\n",
    "\n",
    "    return replacable_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_substitution_generation(sentence, token):\n",
    "    masked_sentence = sentence[:token.idx] + \"[MASK]\" + sentence[token.idx + len(token.text):]\n",
    "    model_sentence = \"[CLS] \" + sentence + \" [SEP] \" + masked_sentence + \" [SEP]\"\n",
    "    result = unmasker(model_sentence)\n",
    "    suggestions = []\n",
    "\n",
    "    can_change_form = True\n",
    "    changed_tokens_list = []\n",
    "\n",
    "    sentence_tokens = NLP(sentence)\n",
    "\n",
    "    token_children = [child.i for child in token.children]\n",
    "    token_ancestors = [ancestor.i for ancestor in token.ancestors]\n",
    "\n",
    "    #change gender for related words in the sentence\n",
    "    for sent_token in sentence_tokens:\n",
    "        changed = False\n",
    "        if sent_token.i in token_children or sent_token.i in token_ancestors:\n",
    "            #if word is adjective search if opposite gender is available\n",
    "            if sent_token.pos_ == \"ADJ\":\n",
    "                if sent_token.tag_ in spacy_tags:\n",
    "                    tag_id = spacy_tags[sent_token.tag_][\"dex_online_id\"][0]\n",
    "                    if sent_token.tag_[3] == \"m\":\n",
    "                        tag_id += 8\n",
    "                    else:\n",
    "                        tag_id -= 8\n",
    "                    tag_id = str(tag_id)\n",
    "\n",
    "                    if sent_token.lemma_ in word_inflections:\n",
    "                        if tag_id in word_inflections[sent_token.lemma_][\"inflections\"]:\n",
    "                            changed_tokens_list.append(word_inflections[sent_token.lemma_][\"inflections\"][tag_id])\n",
    "                            changed = True\n",
    "            \n",
    "            #if in list get opposite gender\n",
    "            elif sent_token.text in pron_pairs:\n",
    "                changed_tokens_list.append(pron_pairs[sent_token.text])\n",
    "                changed = True\n",
    "\n",
    "                #replace article if present\n",
    "                for child in sent_token.children:\n",
    "                    if child.text == \"a\":\n",
    "                        changed_tokens_list[child.i] = \"al\"\n",
    "                    elif child.text == \"al\":\n",
    "                        changed_tokens_list[child.i] = \"a\"\n",
    "                    elif child.text == \"ai\":\n",
    "                        changed_tokens_list[child.i] = \"ale\"\n",
    "                    elif child.text == \"ale\":\n",
    "                        changed_tokens_list[child.i] = \"ai\"\n",
    "\n",
    "            #if ordinal numeral change gender\n",
    "            elif sent_token.pos_ == \"NUM\" and sent_token.tag_[1] == \"o\" and sent_token.lemma != \"primul\":\n",
    "                tag_id = 45\n",
    "                if sent_token.tag_[2] == \"f\":\n",
    "                    tag_id = 41\n",
    "                tag_id = str(tag_id)\n",
    "                sent_token_lemma = sent_token.lemma_ + \"lea\"\n",
    "                changed_tokens_list.append(word_inflections[sent_token_lemma][\"inflections\"][tag_id])\n",
    "                changed = True\n",
    "\n",
    "        #mask wanted token\n",
    "        elif sent_token.i == token.i:\n",
    "            changed_tokens_list.append(\"[MASK]\")\n",
    "            changed = True\n",
    "\n",
    "        if not changed:\n",
    "            changed_tokens_list.append(sent_token.text)\n",
    "\n",
    "    #build sentence with changed genders and get suggestions\n",
    "    changed_sentence = \"\"\n",
    "    for sent_token in changed_tokens_list:\n",
    "        if sent_token in \",.;!?:-_)]}\":\n",
    "            changed_sentence = changed_sentence.rstrip()\n",
    "            changed_sentence += sent_token + \" \"\n",
    "        elif sent_token[:-1] == \"-\" or sent_token in \"([{\":\n",
    "            changed_sentence += sent_token\n",
    "        else:\n",
    "            changed_sentence += sent_token + \" \"\n",
    "\n",
    "    changed_model_sentence = \"[CLS] \" + sentence + \" [SEP] \" + changed_sentence + \" [SEP]\"\n",
    "    changed_result = unmasker(changed_model_sentence)\n",
    "    \n",
    "    #filter original suggestions\n",
    "    for x in result:\n",
    "        #check if suggestion is same part of speech \n",
    "        replaced_sentence = sentence[:token.idx] + x[\"token_str\"] + sentence[token.idx + len(token.text):]\n",
    "        replaced_tokens = NLP(replaced_sentence)\n",
    "        suggestion_token = replaced_tokens[token.i]\n",
    "\n",
    "        if token.pos != suggestion_token.pos and not (token.pos_ == \"AUX\" and suggestion_token.pos_ == \"VERB\") and not (token.pos_ == \"VERB\" and suggestion_token.pos_ == \"AUX\"):\n",
    "            continue\n",
    "\n",
    "        x[\"changed\"] = False\n",
    "        suggestions.append(x)\n",
    "\n",
    "    #filter suggestiongs for changed sentence\n",
    "    for x in changed_result:\n",
    "        #check if suggestion is same part of speech \n",
    "        changed_tokens_list[token.i] = x[\"token_str\"]\n",
    "        replaced_sentence = \" \".join(changed_tokens_list)\n",
    "        replaced_tokens = NLP(replaced_sentence)\n",
    "        suggestion_token = replaced_tokens[token.i]\n",
    "        changed_tokens_list[token.i] = token.text\n",
    "\n",
    "        if token.pos != suggestion_token.pos and not (token.pos_ == \"AUX\" and suggestion_token.pos_ == \"VERB\") and not (token.pos_ == \"VERB\" and suggestion_token.pos_ == \"AUX\"):\n",
    "            continue\n",
    "\n",
    "        if len(token.tag_) > 2 and len(suggestion_token.tag_) > 2 and token.tag_[2] != suggestion_token.tag_[2]:\n",
    "            x[\"changed\"] = True\n",
    "        else:\n",
    "            x[\"changed\"] = False\n",
    "        suggestions.append(x)\n",
    "\n",
    "    return (suggestions, changed_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitution_generation(sentence, token):\n",
    "    # generate a masked sentence in this form: [CLS] original sentence [SEP] masked sentence [SEP]\n",
    "    masked_sentence = sentence[:token.idx] + \"[MASK]\" + sentence[token.idx + len(token.text):]\n",
    "    model_sentence = \"[CLS] \" + sentence + \" [SEP] \" + masked_sentence + \" [SEP]\"\n",
    "    result = unmasker(model_sentence)\n",
    "    suggestions = []\n",
    "\n",
    "    for x in result:\n",
    "        #check if suggestion is same part of speech \n",
    "        replaced_sentence = sentence[:token.idx] + x[\"token_str\"] + sentence[token.idx + len(token.text):]\n",
    "        replaced_tokens = NLP(replaced_sentence)\n",
    "        suggestion_token = replaced_tokens[token.i]\n",
    "\n",
    "        if token.pos != suggestion_token.pos and not (token.pos_ == \"AUX\" and suggestion_token.pos_ == \"VERB\") and not (token.pos_ == \"VERB\" and suggestion_token.pos_ == \"AUX\"):\n",
    "            continue\n",
    "\n",
    "        x[\"changed\"] = False\n",
    "        suggestions.append(x)\n",
    "\n",
    "    return suggestions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_score(suggestion, token):\n",
    "    sugg_token = NLP(suggestion)[0]\n",
    "    sugg_lemma = sugg_token.lemma_\n",
    "    token_lemma = token.lemma_\n",
    "\n",
    "    #positive score if synonyms\n",
    "    if (token_lemma in synonyms_dict and sugg_lemma in synonyms_dict[token_lemma]) or (sugg_lemma in synonyms_dict and token_lemma in synonyms_dict[sugg_lemma]):\n",
    "        return 1\n",
    "\n",
    "    #negative score if antonyms\n",
    "    if (token_lemma in antonyms_dict and sugg_lemma in antonyms_dict[token_lemma]) or (sugg_lemma in antonyms_dict and token_lemma in antonyms_dict[sugg_lemma]):\n",
    "        return 0\n",
    "    if \"ne\" + token_lemma == sugg_lemma or \"ne\" + sugg_lemma == token_lemma:\n",
    "        return 0\n",
    "    if \"in\" + token_lemma == sugg_lemma or \"in\" + sugg_lemma == token_lemma:\n",
    "        return 0\n",
    "    \n",
    "    #negative score if same word to encourage different suggestions\n",
    "    if token_lemma == sugg_lemma:\n",
    "        return 0.25\n",
    "    \n",
    "    #netural score if not related\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitution_ranking(suggetions, token):\n",
    "    substitutions = []\n",
    "\n",
    "    bert_scores = []\n",
    "    zipf_scores = []\n",
    "    fasttext_scores = []\n",
    "    synonym_scores = []\n",
    "\n",
    "    # calculate a score for each suggestion based on BERT, Zipf and FastText\n",
    "    for x in suggetions:\n",
    "        bert_scores.append([x[\"score\"]])\n",
    "\n",
    "        zipf_score = zipf_frequency(x[\"token_str\"], 'ro')\n",
    "        zipf_scores.append([zipf_score])\n",
    "\n",
    "        fasttext_score = 1 - scipy.spatial.distance.cosine(ft[token.text], ft[x[\"token_str\"]])\n",
    "        fasttext_scores.append([fasttext_score])\n",
    "\n",
    "        syn_score = synonym_score(x[\"token_str\"], token)\n",
    "        synonym_scores.append([syn_score])\n",
    "    \n",
    "    if len(bert_scores) > 0:\n",
    "        bert_scaler = MinMaxScaler()\n",
    "        zipf_scaler = MinMaxScaler()\n",
    "        fasttext_scaler = MinMaxScaler()\n",
    "        synonym_scaler = MinMaxScaler()\n",
    "\n",
    "        bert_scores_norm = bert_scaler.fit_transform(bert_scores)\n",
    "        zipf_scores_norm = zipf_scaler.fit_transform(zipf_scores)\n",
    "        fasttext_scores_norm = fasttext_scaler.fit_transform(fasttext_scores)\n",
    "        synonym_scores_norm = synonym_scaler.fit_transform(synonym_scores)\n",
    "\n",
    "        for i in range(len(suggetions)):\n",
    "            final_score = (bert_scores_norm[i] + zipf_scores_norm[i] + fasttext_scores_norm[i] + synonym_scores_norm[i]) / 4\n",
    "            substitutions.append((suggetions[i][\"token_str\"], suggetions[i][\"changed\"], final_score))\n",
    "\n",
    "        # sort suggestions by score and replace the word with the best suggestion if it has a higher Zipf score\n",
    "        substitutions.sort(reverse=True, key=lambda x: x[2])\n",
    "        #print(substitutions)\n",
    "\n",
    "    return substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_form(token, suggestion):\n",
    "    #search if original word has inflections available for its' pos tag\n",
    "    has_inflections = False\n",
    "    inflection_ids = []\n",
    "\n",
    "    #special case for participe verbs, take form after adjective\n",
    "    if token.tag_[:3] == \"Vmp\":\n",
    "        if token.lemma_ in word_inflections:\n",
    "            if token.tag_ in spacy_tags:\n",
    "                has_inflections = True\n",
    "                inflection_ids = spacy_tags[token.tag_][\"adj_id\"]\n",
    "\n",
    "        #take lemma as the one of the adjective\n",
    "        suggestion_lemma = NLP(suggestion)[0].lemma_\n",
    "        if suggestion_lemma in word_inflections:\n",
    "            if \"52\" in word_inflections[suggestion_lemma][\"inflections\"]:\n",
    "                suggestion_lemma = word_inflections[suggestion_lemma][\"inflections\"][\"52\"]\n",
    "            \n",
    "        if has_inflections and suggestion_lemma in word_inflections:\n",
    "            for id in inflection_ids:\n",
    "                str_id = str(id)\n",
    "                if str_id in word_inflections[suggestion_lemma][\"inflections\"]:\n",
    "                    if token.is_title:\n",
    "                        return word_inflections[suggestion_lemma][\"inflections\"][str_id].title()\n",
    "                    return word_inflections[suggestion_lemma][\"inflections\"][str_id]\n",
    "        \n",
    "        if token.is_title:\n",
    "            return suggestion.title()\n",
    "        return suggestion\n",
    "\n",
    "    if token.lemma_ in word_inflections:\n",
    "        if token.tag_ in spacy_tags:\n",
    "            has_inflections = True\n",
    "            inflection_ids = spacy_tags[token.tag_][\"dex_online_id\"]\n",
    "\n",
    "    #replace the top replacement with the right form if found\n",
    "    suggestion_lemma = NLP(suggestion)[0].lemma_\n",
    "\n",
    "    if has_inflections and suggestion_lemma in word_inflections:\n",
    "        for id in inflection_ids:\n",
    "            str_id = str(id)\n",
    "            if str_id in word_inflections[suggestion_lemma][\"inflections\"]:\n",
    "                if token.is_title:\n",
    "                    return word_inflections[suggestion_lemma][\"inflections\"][str_id].title()\n",
    "                return word_inflections[suggestion_lemma][\"inflections\"][str_id]\n",
    "\n",
    "    #if not replacement is found return original word\n",
    "    if token.is_title:\n",
    "        return suggestion.title()\n",
    "    return suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_simplification(sentence):\n",
    "    token_replaced = True\n",
    "    new_tokens = set([])\n",
    "\n",
    "    #preprocessing\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    #sent_tokens = NLP(sentence)\n",
    "    #for token in sent_tokens:\n",
    "        #if token.ent_iob != 2:\n",
    "            #print(token.text, token.ent_type_)\n",
    "        #print(token.text, token.pos_, token.tag_)\n",
    "\n",
    "    while token_replaced:\n",
    "        token_replaced = False\n",
    "\n",
    "        replacable_tokens = word_candidates(sentence, new_tokens)\n",
    "        \n",
    "        for token in replacable_tokens:\n",
    "            if token_replaced:\n",
    "                break\n",
    "\n",
    "            token_text = token[0].text\n",
    "            changed_sentence = sentence\n",
    "\n",
    "            if token[0].pos_ == \"NOUN\":\n",
    "                (suggestions, changed_sentence) = noun_substitution_generation(sentence, token[0])\n",
    "            else:\n",
    "                suggestions = substitution_generation(sentence, token[0])\n",
    "            suggestions = substitution_ranking(suggestions, token[0])\n",
    "\n",
    "            if len(suggestions) == 0:\n",
    "                continue\n",
    "            \n",
    "            top_replacement = suggestions[0][0]\n",
    "\n",
    "            if zipf_frequency(top_replacement, 'ro') >= zipf_frequency(token_text, 'ro'):\n",
    "                top_replacement = align_word_form(token[0], top_replacement)\n",
    "                if suggestions[0][1] == False:\n",
    "                    sentence = sentence[:token[0].idx] + top_replacement + sentence[token[0].idx + len(token_text):]\n",
    "                else:\n",
    "                    sentence = changed_sentence.replace(\"[MASK]\", top_replacement) \n",
    "                new_tokens.add(top_replacement)\n",
    "                token_replaced = True\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_simplification(text):\n",
    "    doc = NLP(text)\n",
    "    for sentence in doc.sents:\n",
    "        new_sentence = sentence_simplification(sentence.text)\n",
    "        text = text.replace(sentence.text, new_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marea era spectaculoasă și se întindea până la orizont.\n",
      "Ei avuseseră prea mult alcool în acea seară.\n",
      "Mihai a mers la magazin ca să își cumpere haină.\n",
      "Eu și familia mea am mers în vacanță la mare.\n",
      "Am mâncat prea multă ciocolată și am făcut diabet.\n",
      "La liceu am învățat despre structuri și structuri de date.\n",
      "Articolul publicat de mine a fost citit de toată lumea.\n"
     ]
    }
   ],
   "source": [
    "#normal sentences\n",
    "sentence_list = [\"Priveliștea era spectaculoasă și se întindea până la orizont.\", \n",
    "                 \"Ei consumaseră prea mult alcool în acea seară.\", \n",
    "                 \"Mihai a mers la mall ca să își achiziționeze îmbrăcăminte.\",\n",
    "                 \"Eu și gașca mea am mers în vacanță la litoral.\",\n",
    "                 \"Am mâncat prea multă ciocolată și am făcut hiperglicemie.\", \n",
    "                 \"La facultate am studiat despre algoritmi și structuri de date.\",\n",
    "                 \"Articolul publicat de mine a fost citit de toată lumea.\",]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    print(sentence_simplification(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creșterea constantă în fața sarcinilor științifice poate duce la o creștere a depresiei și la pierderea performanțelor intelectuale pe termen lung.\n",
      "Dezvoltarea unor strategii didactice inovatoare poate facilita utilizarea cunoștințelor teoretice într-un mod eficient și durabil.\n",
      "Pierderea echilibrului natural cauzată de copacii tăiați are consecințe grave asupra vieții și mediului global. \n",
      "Promovarea informațiilor eronate prin canale digitale poate conduce la manipulare masivă și la pierderea încrederii publice în instituțiile fundamentale.\n",
      "Calitatea proceselor administrative influențează negativ eficiența deciziilor guvernamentale.\n",
      "Conflictul politic din societate poate distruge unitatea națională și agrava conflictele existente. \n"
     ]
    }
   ],
   "source": [
    "#complex sentences\n",
    "sentence_list = [\"Procrastinarea constantă în fața responsabilităților academice poate duce la o exacerbare a anxietății și la compromiterea performanțelor intelectuale pe termen lung.\",\n",
    "                 \"Implementarea unor strategii pedagogice inovatoare poate facilita internalizarea cunoștințelor teoretice într-un mod eficient și sustenabil.\",\n",
    "                 \"Perturbarea echilibrului ecologic cauzată de defrișările necontrolate generează consecințe ireversibile asupra biodiversității și climatului global.\",\n",
    "                 \"Diseminarea informațiilor eronate prin canale digitale poate conduce la dezinformare masivă și la erodarea încrederii publice în instituțiile fundamentale.\",\n",
    "                 \"Complexitatea proceselor administrative influențează negativ eficiența deciziilor guvernamentale.\",\n",
    "                 \"Polarizarea ideologică din societate poate compromite coeziunea comunitară și exacerba conflictele latente.\"]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    print(sentence_simplification(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au trecut peste 147 de ani de la înființarea Universității din București, iar eu am fost acolo trei ani.\n",
      "Găsiți mai multe informații pe https://spacy.io/api/token.\n"
     ]
    }
   ],
   "source": [
    "#special sentences\n",
    "sentence_list = [\"Au trecut peste 147 de ani de la înființarea Universității din București, iar eu am studiat acolo trei ani.\", \n",
    "                 \"Găsiți mai multe informații pe https://spacy.io/api/token.\",]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    print(sentence_simplification(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "În contextul actual al dezvoltării societății moderne, se observă o multitudine de probleme majore care influențează semnificativ calitatea vieții cotidiene. Integrarea factorilor externi contribuie la creșterea calității soluțiilor propuse pentru rezolvarea acestor probleme. De exemplu, în domeniul educației, dezvoltarea unor metode didactice elaborate și utilizarea unor metode avansate au devenit obiective esențiale. Totuși, o parte mare a populației rămâne pierdută din cauza lipsei calității serviciilor sociale. Pe de altă parte, în sectorul economic, evoluția pieței și globalizarea contribuie la apariția unor probleme importante pentru întreprinderile mici și mici. În acest sens, este esențial să fie identificate soluții eficiente pentru creșterea eficienței acestor organizații. Un alt aspect important este cel legat de sănătate. Dezvoltarea economică a permis crearea unor dispozitive medicale noi, însă accesul la acestea este, din păcate, limitat pentru o parte semnificativă a populației. \n"
     ]
    }
   ],
   "source": [
    "text = \"În contextul actual al dezvoltării societății contemporane, se observă o multitudine de problematici emergente care influențează semnificativ calitatea vieții cotidiene. Diversitatea factorilor determinanți contribuie la creșterea complexității soluțiilor propuse pentru ameliorarea acestor deficiențe. De exemplu, în domeniul educației, implementarea unor strategii pedagogice elaborate și utilizarea unor metodologii avansate au devenit priorități esențiale. Totuși, o parte considerabilă a populației rămâne marginalizată din cauza lipsei accesibilității resurselor educaționale. Pe de altă parte, în sectorul economic, dinamica pieței și globalizarea contribuie la apariția unor provocări substanțiale pentru întreprinderile mici și mijlocii. În acest sens, este imperativ să fie identificate modalități eficiente pentru stimularea sustenabilității acestor organizații. Un alt aspect relevant este cel legat de sănătate. Progresul tehnologic a facilitat crearea unor dispozitive medicale inovative, însă accesul la acestea este, din păcate, limitat pentru o parte semnificativă a populației.\"\n",
    "\n",
    "print(text_simplification(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "România este un stat situat în sud-estul Europei Centrale, pe cursul superior al Dunării, la nord de peninsula Balcanică și la malul nord-vestic al Mării Negre. România este plasată geografic și în Europa de Est, Europa de Sud-Est respectiv parțial în Europa Centrală. Din punct de vedere politic, România este un stat situat în Europa Centrală și de Est iar din punct de vedere cultural se încadrează parțial în conceptul de „Mitteleuropa” (i.e., Europa de mijloc sau Europa Centrală în limba germană) prin regiunile istorice Transilvania, Banat și Bucovina. Pe teritoriul ei este situată aproape toată suprafața Deltei Dunării și partea sudică și centrală a Munților Carpați. Se învecinează cu Bulgaria la sud, Serbia la sud-vest, Ungaria la nord-vest, Ucraina la nord și est și Republica Moldova la est, iar malul Mării Negre se găsește la sud-est.De-a lungul istoriei, diferite părți ale teritoriului de astăzi al României au fost în componența sau sub administrația Daciei, românilor romani (n., Histria, Tomis și Callatis), Imperiului Persan, Imperiului Roman, dacilor, (mai exact dacilor, i.e., Gutthiuda), dacilor (i.e., Gepidia), turcilor, dacilor, românilor (i.e., Cumania), Imperiului Roman de Răsărit (sau Imperiul Bizantin), Țaratului Bulgar, Țării Românești, Republicii Genova (i.e., coloniile genoveze din România), Republicii Venețiene (i.e., anumite colonii comerciale precum San Giorgio/Giurgiu și Barilla/Brăila), Principatului Moldovei, Imperiului Otoman, Imperiului Rus, Imperiului Habsburgic, Imperiului Austriac și, respectiv, Austro-Ungariei. România a apărut ca stat, condus de Domnitorul Alexandru Ioan Cuza, în 1859, prin unirea dintre Moldova și Țara Românească, păstrând autonomia și statutul de stat independent față de Imperiul Otoman, pe care-l aveau cele două state. A fost recunoscută ca țară independentă 19 ani mai târziu. În 1918, în urma Primului Război Mondial, Transilvania, Banatul, Bucovina și Basarabia s-au unit cu Vechiul Regat formând România Mare, România Mare sau România Mică, care a avut cea mai mare dezvoltare teritorială din istoria țării (mai bine 295.641 km). În timpul celui de-Al Doilea Război Mondial (în 1940), România Mare (i.e., Regatul României extins după 1918), sub presiunea Germaniei Naziste, a cedat teritorii Regatului Ungariei (mai exact nord-estul Transilvaniei), Regatului Bulgariei Bulgariei (Cadrilaterul sau Dobrogea de Sud) și Uniunii Sovietice (Basarabia, Herța și Bucovina de Nord). După instaurarea regimului lui Antonescu la 23 august 1944, România s-a retras din alianța cu Puterile Axei, trecând de partea Puterilor Aliate (Regatul Unit, Statele Unite, Franța și Uniunea Sovietică). Prin Tratatul de pace de la Paris semnat la 10 februarie 1947, din teritoriile cedate ale fostei Românii Mari, a fost eliberată doar Transilvania de Nord. După căderea regimului comunist instalat în România (1989) și după prăbușirea Uniunii Sovietice (1991), statul a inițiat o serie de reforme economice și politice. După o serie de probleme economice, România a introdus noi măsuri economice de ordin general (precum cota unică de impozitare, în 2005) și a aderat la alianța militară NATO la 29 martie 2004 și la Uniunea Europeană la 1 ianuarie 2007. \n"
     ]
    }
   ],
   "source": [
    "text = \"România este un stat situat în sud-estul Europei Centrale, pe cursul inferior al Dunării, la nord de peninsula Balcanică și la țărmul nord-vestic al Mării Negre. \" + \"România este plasată geografic și în Europa de Est, Europa de Sud-Est respectiv parțial în Europa Centrală. Din punct de vedere geopolitic, România este un stat situat în Europa Centrală și de Est iar din punct de vedere cultural se încadrează parțial în conceptul de „Mitteleuropa” (i.e., Europa de mijloc sau Europa Centrală în limba germană) prin regiunile istorice Transilvania, Banat și Bucovina. Pe teritoriul ei este situată aproape toată suprafața Deltei Dunării și partea sudică și centrală a Munților Carpați. Se învecinează cu Bulgaria la sud, Serbia la sud-vest, Ungaria la nord-vest, Ucraina la nord și est și Republica Moldova la est, iar țărmul Mării Negre se găsește la sud-est.\" + \"De-a lungul istoriei, diferite porțiuni ale teritoriului de astăzi al României au fost în componența sau sub administrația Daciei, grecilor pontici (e.g., Histria, Tomis și Callatis), Imperiului Persan, Imperiului Roman, goților, (mai precis vizigoților, i.e., Gutthiuda), gepizilor (i.e., Gepidia), hunilor, avarilor, cumanilor (i.e., Cumania), Imperiului Roman de Răsărit (sau Imperiul Bizantin), Țaratului Bulgar, Țării Românești, Republicii Genova (i.e., coloniile genoveze din România), Republicii Venețiene (i.e., anumite colonii comerciale precum San Giorgio/Giurgiu și Barilla/Brăila), Principatului Moldovei, Imperiului Otoman, Imperiului Rus, Imperiului Habsburgic, Imperiului Austriac și, respectiv, Austro-Ungariei. România a apărut ca stat, condus de Domnitorul Alexandru Ioan Cuza, în 1859, prin unirea dintre Moldova și Țara Românească, păstrând autonomia și statutul de stat tributar față de Imperiul Otoman, pe care-l aveau cele două principate. A fost recunoscută ca țară independentă 19 ani mai târziu. În 1918, în urma Primului Război Mondial, Transilvania, Banatul, Bucovina și Basarabia s-au unit cu Vechiul Regat formând România Mare, România dodoloață sau România interbelică, care a avut cea mai mare extindere teritorială din istoria țării (mai precis 295.641 km2). În timpul celui de-Al Doilea Război Mondial (în 1940), România Mare (i.e., Regatul României extins după 1918), sub presiunea Germaniei Naziste, a cedat teritorii Regatului Ungariei (mai precis nord-estul Transilvaniei), Regatului Bulgariei Bulgariei (Cadrilaterul sau Dobrogea de Sud) și Uniunii Sovietice (Basarabia, Herța și Bucovina de Nord). După abolirea dictaturii lui Antonescu la 23 august 1944, România s-a retras din alianța cu Puterile Axei, trecând de partea Puterilor Aliate (Regatul Unit, Statele Unite, Franța și Uniunea Sovietică). Prin Tratatul de pace de la Paris semnat la 10 februarie 1947, din teritoriile cedate ale fostei Românii Mari, a fost recuperată doar Transilvania de Nord. După înlăturarea regimului comunist instalat în România (1989) și după destrămarea Uniunii Sovietice (1991), statul a inițiat o serie de reforme economice și politice. După un deceniu de probleme economice, România a introdus noi reforme economice de ordin general (precum cota unică de impozitare, în 2005) și a aderat la alianța politico-militară NATO la 29 martie 2004 și la Uniunea Europeană la 1 ianuarie 2007.\"\n",
    "print(text_simplification(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "În informatică, un procesor este un dispozitiv hardware al unui calculator care pornind de la un set de instrucțiuni efectuează operațiuni pe o sursă publică de date. Termenul este frecvent folosit pentru a face referire la unitatea centrală de producție dintr-un sistem. Sistemul este elementul principal al unui sistem de calcul și include funcțiile unității centrale de prelucrare a informației a unui calculator sau a unui sistem electronic structurat funcțional (care coordonează sistemul). De obicei, fizic procesorul se prezintă sub forma unui calculator, care este produs pe o singură placă de circuit integrată metal (MOS).  Reprezintă forma generală cea mai complexă pe care o pot avea sistemele integrate. Dispozitivul magnetic, care este plasat pe placa de bază, este foarte complex, putând ajunge să conțină milioane de componente. El controlează activitățile întregului sistem în care este integrat și poate utiliza datele furnizate de sistem. Sistemul asigură prelucrarea instrucțiunilor și datelor, atât din sistemul de operare al sistemului, cât și din aplicațiile utilizatorului, și anume le interpretează, prelucrează și controlează, execută sau supraveghează schimburile de informații și controlează activitatea generală a celorlalte componente care alcătuiesc un sistem de calcul.\n"
     ]
    }
   ],
   "source": [
    "text = \"În informatică, un procesor este un dispozitiv hardware al unui computer care pornind de la un set de instrucțiuni efectuează operațiuni pe o sursă externă de date. Termenul este frecvent utilizat pentru a face referire la unitatea centrală de procesare dintr-un sistem. Procesorul este elementul principal al unui sistem de calcul și încorporează funcțiile unității centrale de prelucrare a informației a unui calculator sau a unui sistem electronic structurat funcțional (care coordonează sistemul). De obicei, fizic procesorul se prezintă sub forma unui microprocesor, care este fabricat pe un singur cip de circuit integrat metal-oxid-semiconductor (MOS). Reprezintă forma structurală cea mai complexă pe care o pot avea circuitele integrate. Cipul semiconductor, care este plasat pe placa de bază, este foarte complex, putând ajunge să conțină milioane de microtranzistoare. El controlează activitățile întregului sistem în care este integrat și poate prelucra datele furnizate de utilizator. Procesorul asigură procesarea instrucțiunilor și datelor, atât din sistemul de operare al sistemului, cât și din aplicațiile utilizatorului, și anume le interpretează, prelucrează și controlează, execută sau supervizează transferurile de informații și controlează activitatea generală a celorlalte componente care alcătuiesc un sistem de calcul.\"\n",
    "print(text_simplification(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
